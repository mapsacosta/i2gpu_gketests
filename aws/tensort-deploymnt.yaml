apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
  labels:
    app: inference-server-aws-test
    release: test
  name: tensorrt-inference-server-aws-test
  namespace: default
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: inference-server-aws-test
      release: test
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: inference-server-aws-test
        release: test
    spec:
      serviceAccountName: s3-read
      containers:
        #      - command:
        #  - /bin/sh
        #  - -ec
        #  - while :; do echo '.'; sleep 5 ; done
      - args:
          - trtserver
          - --model-store=s3://ailab-cloud-model-repo/model_repository
          - --allow-metrics=true
          - --log-verbose=5
        env:
          - name: AWS_REGION
            value: us-west-2
          - name: AWS_CONFIG_FILE
            value: '/opt/tensorrtserver/aws/config'
          - name: AWS_DEFAULT_REGION
            value: us-west-2
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-access-key-id
                key: aws_access_key_id
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-secret-access-key
                key: aws_secret_access_key
        image: nvcr.io/nvidia/tensorrtserver:20.01-py3
        imagePullPolicy: Always
        name: tensorrt-inference-server-aws-test
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        - containerPort: 8001
          name: grpc
          protocol: TCP
        - containerPort: 8002
          name: metrics
          protocol: TCP
        volumeMounts:
        # name must match the volume name below
          - name: creds-volume
            mountPath: /opt/tensorrtserver/aws/credentials
            subPath: credentials
          - name: conf-volume
            mountPath: /opt/tensorrtserver/aws/config
            subPath: config
        resources:
          limits:
            cpu: "16"
            memory: 16Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "3"
            memory: 5Gi
            nvidia.com/gpu: "1"
        securityContext:
          runAsUser: 1000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      volumes:
        - name: creds-volume
          secret:
            secretName: aws-credentials
        - name: conf-volume
          secret:
            secretName: aws-config
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: inference-svc
  namespace: default
  labels:
    app: inference-server-aws-test
spec:
  type: LoadBalancer
  ports:
    - port: 8000
      targetPort: http
      name: http-inference-server
    - port: 8001
      targetPort: grpc
      name: grpc-inference-server
    - port: 8002
      targetPort: metrics
      name: metrics-inference-server
  selector:
    app: inference-server-aws-test
