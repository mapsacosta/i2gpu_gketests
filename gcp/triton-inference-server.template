apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ${server_name}-server
    owner: ${owner}
  name: ${server_name}-inference-server
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ${server_name}-server
      owner: ${owner}
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: ${server_name}-server
        owner: ${owner}
    spec:
      containers:
      - args:
        - tritonserver
        - --model-store=${model_dir}
        - --allow-metrics=true
        - --log-verbose=5
        image: nvcr.io/nvidia/tritonserver:21.05-py3
        imagePullPolicy: Always
        name: ${server_name}-inference-server
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        - containerPort: 8001
          name: grpc
          protocol: TCP
        - containerPort: 8002
          name: metrics
          protocol: TCP
        resources:
          limits:
            cpu: "15"
            memory: 60Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "12"
            memory: 32Gi
            nvidia.com/gpu: "1"
        securityContext:
          runAsUser: 1000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /secret
          name: vsecret
          readOnly: true
        - mountPath: /dev/shm
          name: dshm
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - name: vsecret
        secret:
          defaultMode: 420
          secretName: gcpcreds
      - emptyDir:
          medium: Memory
        name: dshm
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ${server_name}-server
  name: ${server_name}-svc
  namespace: default
spec:
  externalTrafficPolicy: Cluster
  ports:
  - name: http-inference-server
    port: 8000
    protocol: TCP
    targetPort: http
  - name: grpc-inference-server
    port: 8001
    protocol: TCP
    targetPort: grpc
  - name: metrics-inference-server
    port: 8002
    protocol: TCP
    targetPort: metrics
  selector:
    app: ${server_name}-server
    owner: ${owner}
  sessionAffinity: None
  type: LoadBalancer
